{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "old_nltk_data = '/Users/liling.tan/nltk_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ABC corpus.\n",
    "with io.open(old_nltk_data+'corpora/abc/rural.txt') as fin:\n",
    "    rural_texts = [line.strip() for line in fin if line.strip()]\n",
    "with io.open(old_nltk_data+'corpora/abc/science.txt', encoding='latin_1') as fin:\n",
    "    science_texts = [line.strip().encode('utf8').decode('utf8') for line in fin if \n",
    "                    line.strip().encode('utf8').decode('utf8')]\n",
    "\n",
    "rural_df = pd.DataFrame({'text':rural_texts})\n",
    "rural_df['subcorpora'] = 'Rural News'\n",
    "\n",
    "science_df = pd.DataFrame({'text':science_texts})\n",
    "science_df['subcorpora'] = 'Science News'\n",
    "\n",
    "df_abc = pd.concat([rural_df, science_df])\n",
    "df_abc.to_csv('nltk_data/corpora/abc.tsv', sep='\\t', index=False)\n",
    "df_abc = pd.read_csv('nltk_data/corpora/abc.tsv', sep='\\t', \n",
    "                     dtype={'text':str, 'subcorpora':str})\n",
    "\n",
    "abc_meta = {'title':'Australian Broadcasting Commission 2006',\n",
    "            'source': 'http://www.abc.net.au/',\n",
    "            'subcorpora': {'Rural News': {'source': 'http://www.abc.net.au/rural/news/'},\n",
    "                           'Science News': {'source': 'http://www.abc.net.au/science/news/'}\n",
    "                          }\n",
    "           }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brown\n",
    "with open(old_nltk_data+'corpora/brown/cats.txt') as fin:\n",
    "     categories = {line.strip().split(' ')[0]:line.strip().split(' ')[1] \n",
    "                   for line in fin}\n",
    "        \n",
    "brown_dir = old_nltk_data+'corpora/brown/'\n",
    "\n",
    "rows = []\n",
    "for filename in os.listdir(brown_dir):\n",
    "    if filename in ['CONTENTS', 'cats.txt', 'README']:\n",
    "        continue\n",
    "    cat = categories[filename]\n",
    "    with open(brown_dir+filename) as fin:\n",
    "        i = -1\n",
    "        for paragraph in fin.read().split('\\n\\n'):\n",
    "            if not paragraph.strip():\n",
    "                continue\n",
    "            i += 1\n",
    "            j = -1\n",
    "            for sent in paragraph.split('\\n'):\n",
    "                if not sent.strip():\n",
    "                    continue\n",
    "                j += 1\n",
    "                raw = sent.strip()\n",
    "                text, pos = zip(*[word.split('/') for word in raw.split()])\n",
    "                rows.append({'filename': filename, \n",
    "                              'para_id': i, \n",
    "                              'sent_id': j, \n",
    "                              'raw_text': raw, \n",
    "                              'tokenized_text': ' '.join(text), \n",
    "                              'tokenized_pos': ' '.join(pos), \n",
    "                              'label': cat})\n",
    "                \n",
    "df_brown = pd.DataFrame(rows)[['filename', 'para_id', 'sent_id', \n",
    "                              'raw_text', 'tokenized_text', 'tokenized_pos', 'label']]\n",
    "df_brown.to_csv('nltk_data/corpora/brown.tsv', sep='\\t', index=False)\n",
    "df_brown = pd.read_csv('nltk_data/corpora/brown.tsv', sep='\\t', \n",
    "                     dtype={'filename':str, 'para_id':int, 'sent_id':int,\n",
    "                             'raw_text':str, 'tokenized_text':str, 'tokenized_pos':str,\n",
    "                           'label':str})\n",
    "abc_readme = \"\"\"BROWN CORPUS\n",
    "\n",
    "A Standard Corpus of Present-Day Edited American\n",
    "English, for use with Digital Computers.\n",
    "\n",
    "by W. N. Francis and H. Kucera (1964)\n",
    "Department of Linguistics, Brown University\n",
    "Providence, Rhode Island, USA\n",
    "\n",
    "Revised 1971, Revised and Amplified 1979\n",
    "\n",
    "http://www.hit.uib.no/icame/brown/bcm.html\n",
    "\n",
    "Distributed with the permission of the copyright holder,\n",
    "redistribution permitted.\"\"\"\n",
    "\n",
    "brown_meta = {'title':'Brown Corpus',\n",
    "            'description': str('A Standard Corpus of Present-Day Edited American English, '\n",
    "                               'for use with Digital Computers.'),\n",
    "            'authors': 'W. N. Francis and H. Kucera (1964)'\n",
    "            'url': 'http://www.hit.uib.no/icame/brown/bcm.html',\n",
    "            'readme': abc_readme}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gazetteers\n",
    "\n",
    "gazetteers_filename2labels = {'mexstates.txt':'Mexico States',\n",
    "                              'caprovinces.txt': 'Canada Provinces',\n",
    "                              'usstateabbrev.txt': 'US State Abbreviations',\n",
    "                              'uscities.txt': 'US Cities',\n",
    "                              'countries.txt': 'Countries',\n",
    "                              'isocountries.txt': 'Countries ISO codes',\n",
    "                              'nationalities.txt': 'Nationalities',\n",
    "                              'usstates.txt': 'US States'\n",
    "                             }\n",
    "\n",
    "rows = []\n",
    "for filename in os.listdir(old_nltk_data+'corpora/gazetteers/'):\n",
    "    if filename in ['LICENSE.txt']:\n",
    "        continue\n",
    "    label = gazetteers_filename2labels[filename]\n",
    "    with io.open(old_nltk_data+'corpora/gazetteers/'+filename, encoding='ISO-8859-2') as fin:\n",
    "        for line in fin:\n",
    "            if line.strip():\n",
    "                text = line.strip()\n",
    "                if text == 'QuerĂŠtaro':\n",
    "                    text = 'Querétaro'\n",
    "                rows.append({'text':text, 'label':label})\n",
    "\n",
    "df_gazetteers = pd.DataFrame(rows)[['text', 'label']]\n",
    "\n",
    "#alpabet = list('abcdefghijklmnopqrstuvwxyz. ()-,') + list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "#alpabet += [\"'\"]\n",
    "#[word for word in df_gazetteers['text'] if any(ch for ch in word if ch not in alpabet)]\n",
    "\n",
    "df_gazetteers.to_csv('nltk_data/corpora/gazetteers.tsv', sep='\\t', index=False)\n",
    "df_gazetteers = pd.read_csv('nltk_data/corpora/gazetteers.tsv', sep='\\t', \n",
    "                     dtype={'text':str, 'label':str})\n",
    "\n",
    "gazetteers_filename2labels = {'mexstates.txt':'Mexico States',\n",
    "                              'caprovinces.txt': 'Canada Provinces',\n",
    "                              'usstateabbrev.txt': 'US State Abbreviations',\n",
    "                              'uscities.txt': 'US Cities',\n",
    "                              'countries.txt': 'Countries',\n",
    "                              'isocountries.txt': 'Countries ISO codes',\n",
    "                              'nationalities.txt': 'Nationalities',\n",
    "                              'usstates.txt': 'US States'\n",
    "                             }\n",
    "\n",
    "abc_meta = {'title':'Geolocation Gazeteers',\n",
    "            'subcorpora': {'Mexico States': {'original_file': 'mexstates.txt'},\n",
    "                           'Canada Provinces': {'original_file': 'caprovinces.txt'},\n",
    "                           'US State Abbreviations': {'original_file': 'usstates.txt'},\n",
    "                           'US States': {'original_file': 'usstateabbrev.txt'},\n",
    "                           'US Cities': {'original_file':'uscities.txt',\n",
    "                                         'source': 'http://en.wikipedia.org/wiki/List_of_cities_in_the_United_States_with_over_100%2C000_people',\n",
    "                                         'license': 'GNU Free Documentation License',\n",
    "                                         'license_url': 'http://www.gnu.org/copyleft/fdl.html'\n",
    "                                        },\n",
    "                           'Countries': {'original_file':'countries.txt',\n",
    "                                         'source':'http://en.wikipedia.org/wiki/List_of_countries',\n",
    "                                         'license': 'GNU Free Documentation License',\n",
    "                                         'license_url': 'http://www.gnu.org/copyleft/fdl.html'\n",
    "                                        },\n",
    "                           'Countries ISO codes': {'original_file': 'isocountries.txt',\n",
    "                                                  'source': 'http://www.guavastudios.com/country-list.htm'\n",
    "                                                  },\n",
    "                           'Nationalities': {'original_file': 'nationalities.txt',\n",
    "                                            'source': 'http://www.guavastudios.com/nationalities-list.htm'\n",
    "                                            },\n",
    "                          }\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "en_words = []\n",
    "with open(old_nltk_data+'corpora/words/en') as fin:\n",
    "    for line in fin:\n",
    "        en_words.append(line.strip())\n",
    "\n",
    "basic_en_words = []\n",
    "with open(old_nltk_data+'corpora/words/en-basic') as fin:\n",
    "    for line in fin:\n",
    "        basic_en_words.append(line.strip())\n",
    "        \n",
    "words_meta = {'title':'Word Lists',\n",
    "            'subcorpora': {'Unix Words':{'source':'http://en.wikipedia.org/wiki/Words_(Unix)'},\n",
    "                           'Ogden Basic English': {'title': 'The ABC of Basic English',\n",
    "                                                   'author':'C.K. Ogden (1932)'}\n",
    "                          }\n",
    "           }\n",
    "\n",
    "unix_words = pd.DataFrame({'text':en_words})\n",
    "ogden_words = pd.DataFrame({'text':basic_en_words})\n",
    "\n",
    "unix_words.to_csv('nltk_data/corpora/unix_words.tsv', sep='\\t', index=False)\n",
    "ogden_words.to_csv('nltk_data/corpora/ogden_words.tsv', sep='\\t', index=False)\n",
    "\n",
    "\n",
    "unix_words = pd.read_csv('nltk_data/corpora/unix_words.tsv', sep='\\t', dtype={'text':str})\n",
    "ogden_words = pd.read_csv('nltk_data/corpora/ogden_words.tsv', sep='\\t', dtype={'text':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
